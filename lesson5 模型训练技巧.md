# 激活函数
**sigmoid问题：**
1. 饱和区域梯度为0
2. sigmoid输出恒为正。恒为正的数据作为输入数据，将会导致梯度要么都为正，要么都为负。因此w只能通向变化。如果要达到w的最优值，那么所走的路径是很曲折的（比如w1，w2变化方向不一致，要达到的话不能走斜线，只能分两次，竖直和水平各走一次），因此收敛速度很慢。所以我们更希望得到关于原点中心的输入数据和关于原点中心对此的输出
3. 指数运算很耗时

**tanh**
- 输出在[-1，1]之间，解决了sigmoid输出不关于原点对称的问题，但是仍然存在数据饱和的问题

**ReLu**
- 优点：在>0的区域不会饱和，计算很快，而且收敛也比sigmoid和tanh函数快（大约快6倍）
- 缺点：输出不是关于原点对称的，而且如果每个输入数据都处于非激活区域，计算得到梯度为0，权重不会被更新（Dead ReLu），神经网络不会训练。可能原因是初始化不合理或者学习率更大。Dead ReLu不可逆转。一般来说把偏置值初始化为比较小的正值，比如0.01，这会使网络更有可能输出正值，避免Dead ReLu

**Leaky ReLu**
- 优点：不会饱和，计算效率高，收敛快，不会有Dead ReLu的问题
- 缺点：不是关于原点对称

**Exponential Linear Units**
- 优点：不会饱和，收敛快，没有Dead ReLu，输出更接近零均值
- 缺点：需要指数运算，计算代价高

**Maxout**
- 优点：不会饱和，没有Dead ReLu，是ReLu和Leaky ReLu的一种推广
- 缺点：参数数量翻倍

**激活函数的选择和优化方式是有关的**
建议：
1. 使用ReLu，小心地设置学习率
2. 尝试Leaky ReLu、ELU、Maxout
3. 尝试tanh但不要期望太高
4. 不要使用sigmoid（除了在rnn中），因为tanh可以替代它

# 数据预处理

1. 零均值。有两种方式，减去每个像素均值（32x32x3），或者减去每个通道均值（3）
2. 归一化（在cv中不常见，因为像素本身取值是0-255）
3. PCA去除数据相关性，使得协方差矩阵为对角矩阵。再进行白化，使得协方差矩阵为单位矩阵（机器学习中常见，cv不常见，因为图像太多维，协方差矩阵太大）

# 权重初始化

1. 如果随机乘一样的值，那么所有神经元是对称的，网络无法工作。
一般用很小的随机值进行初始化（比如从标准差为0.01的零均值高斯随机变量中产生）。这种方式在层数较少的网络里工作不错，但是层数多的时候，可能导致每层的输出数据分布不同的问题。层数多了之后，标准差越来越小，最终为0，这是因为W是小量，对层之后导致输出趋近0。
如果输出是小量，那么该处的梯度也是小量（~WX，梯度消失），这样导致权重几乎不变。一层层反向传播就更小了，梯度消失
如果使用标准差为1的零均值高斯随机变量产生权重，可能导致饱和。
激活函数为tanh时，一种有效的初始化方式：权重标准差~1/sqrt(num_in)，这样有助于每层的输出标准差为1。
但是上述方式在ReLu中不奏效，因为ReLu让方差减半了（把<0的一办置为0）。因此权重标准差~1/sqrt(num_in/2)。没有这个2，输出分布就会以指数级别坍缩

2. Batch Normalization：对每个维度单独做归一化，来获得单位高斯分布的变量。该层加在全连接层（或者卷积层）和非线性激活层（如tanh）之间。但是问题时tanh的输入一定要是单位高斯分布的吗？可以这么解决
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTkyNTk2ODc0MSwyMDAzODkzMjc0XX0=
-->