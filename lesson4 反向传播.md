**Gradients add up at forks**. The forward expression involves the variables  **x,y**  multiple times, so when we perform backpropagation we must be careful to use  `+=`  instead of  `=`  to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the  _multivariable chain rule_  in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.

_Unintuitive effects and their consequences_. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^Tx_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples $x_i$ by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.

If your weight matrix **W** is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector **z** almost binary: either 1 or 0. But if that is the case, **z*(1-z)**, which is local gradient of the sigmoid non-linearity, will in both cases become **zero** (“vanish”)**,** making the gradient for both **x** and **W** be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExNzA3NzYzNDcsMTAwMDExMjc3N119
-->